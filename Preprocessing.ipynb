{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cats(df):\n",
    "    import pandas as pd\n",
    "    cats = []\n",
    "    for i in range(0,len(df)):\n",
    "        row = df.iloc[i,:]\n",
    "        cats.append(list(row[~pd.isna(row.values)].index))\n",
    "    return pd.Series(cats)\n",
    "\n",
    "def get_lang_detector(nlp, name):\n",
    "    \"\"\"Adds language detection to spaCy pipeline\"\"\"\n",
    "    return LanguageDetector()\n",
    "\n",
    "def fix_ShortComments(df, col = 'comments'):\n",
    "    \"\"\"Replaces comments with three or less characters with 'no comments found'\"\"\"\n",
    "    m = df[col].str.len().values <= 3\n",
    "    df[col] = df[col].mask(m, 'no comment found')\n",
    "    return df\n",
    "\n",
    "def remove_CharRepetition(string):\n",
    "    \"\"\"Reduces 3 or more same character repetition to one: pppleeeease -> please\"\"\"\n",
    "    import re\n",
    "    def extract(match_obj):\n",
    "        if match_obj.group() is not None:\n",
    "            char = match_obj.group(0)[0]\n",
    "        return char\n",
    "    return re.sub(r\"([aeiou])\\1\\1+|([b-df-hj-np-tv-z])\\2\\2+|(\\W)\\3+\", extract, string)\n",
    "\n",
    "def remove_Punctuation(text):\n",
    "    \"\"\"Removes all punctuation symbols but apostrophes, commas and periods\"\"\"\n",
    "    import string, re\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\".\", \"\")\n",
    "    remove = remove.replace(\",\", \"\")\n",
    "    remove = remove.replace(\"'\",\"\")\n",
    "    pattern = r\"[{}]|[{}]$|^[{}]\".format(re.escape(remove), re.escape(string.punctuation), re.escape(string.punctuation))\n",
    "    \n",
    "    return re.sub(pattern, \"\", text)\n",
    "\n",
    "def text_cleaning(df, col = 'comments'):\n",
    "    \"\"\"\n",
    "    1. Replaces any character representing a whitespace\n",
    "    2. Removes extra white spaces\n",
    "    3. Unescapes unicode and html characters\n",
    "    4. Removes web sites and emails\n",
    "    5. Removes white spaces at the begining/end of the comment\n",
    "    6. Removes not word characters\n",
    "    \"\"\"\n",
    "    import html, string\n",
    "    from unidecode import unidecode\n",
    "    return (df[col]\n",
    "            .str.translate(str.maketrans(string.whitespace, \"      \"))# Replaces whitespace characters ' \\t\\n\\r\\x0b\\x0c'\n",
    "            .map(unidecode, na_action = 'ignore')# Decodes characters with unicode encoding\n",
    "            .map(html.unescape, na_action = 'ignore')# Unescapes HTML characters\n",
    "            .replace('[Ff]rom case [a-zA-Z]{4,}\\S*:\\s?|[Tt]o case [a-zA-Z]{4,}\\S*:\\s?','', regex = True) # Removes a web classificator 'From case:'\n",
    "            .replace(\"(https\\W*\\S*|www\\W*\\S*)|(\\S*@\\S*\\s?)\", \"\", regex = True)# Removes websites and emais###(https\\W*\\S*|www\\W+\\S+)|(\\S*@\\S*\\s?)\n",
    "            .apply(remove_CharRepetition)# Removes repetition of 3 or more characters, i.e., pllleeeease --> please\n",
    "            .replace(\"^\\s+|\\s+$\", \"\", regex = True)# Removes extra spaces at the begining/end of the comment\n",
    "            .apply(remove_Punctuation)# Removes every punctuation symbol but apostrophes, commas and periods\n",
    "            .replace(\"\\s{2,}\", \" \", regex = True)# Removes extra whitespaces\n",
    "            .str.lower()\n",
    "            )\n",
    "\n",
    "def generate_Doc(df, col = 'comments'):\n",
    "    import swifter\n",
    "    \"\"\"Generates a spaCy Doc per comment\"\"\"\n",
    "    return df[col].swifter.apply(nlp)\n",
    "\n",
    "def count_Sents(df, col = 'Doc'):\n",
    "    \"\"\"Counts the number of sentences per comment\"\"\"\n",
    "    return [len(list(Doc.sents)) for Doc in df[col]]\n",
    "\n",
    "def get_Lang(df, col, info):\n",
    "    \"\"\"Extracts the language information from nlp object\"\"\"\n",
    "    return [Doc._.language.get(info) for Doc in df[col]]\n",
    "\n",
    "def remove_ShortComments(df, col = 'comments'):\n",
    "    \"\"\"Removes comments whose length is < 2\"\"\"\n",
    "    return df[df[col].apply(len) > 2]\n",
    "\n",
    "def delete_AgentComments(df, col1 = 'comments', col2 = 'Doc'):\n",
    "    \"\"\"\n",
    "    Deletes comments typed by a contact center agent\n",
    "    \n",
    "    The entries are:\n",
    "    \n",
    "    df: the complete data frame\n",
    "    col1: the column containing the comments\n",
    "    col2: the column containing the nlp object\n",
    "    \n",
    "    Returns those elements of the data frame that contain only customer comments based on the grammatical structure of the sentences;\n",
    "    guest/customer word doesn not appear as nominal subject followed by a proper noun\n",
    "    For more information see https://universaldependencies.org/en/dep/nsubj.html    \n",
    "    \"\"\"\n",
    "    from spacy.matcher import Matcher\n",
    "    import swifter\n",
    "    series = df[df[col1].str.contains('guest|customer', regex = True, na = False)][col2]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [[{'LOWER': {'IN':['guest', 'customer']}, 'POS': 'PROPN', 'DEP': 'nsubj'}]]\n",
    "    matcher.add('CustomerGuest', pattern)\n",
    "    matches = series.swifter.progress_bar(False).apply(matcher)\n",
    "    # return pd.Series([True if item != 0 else False for item in matches.apply(len)])\n",
    "    return df[~df.index.isin(matches[matches.apply(len) != 0].index)]\n",
    "\n",
    "def create_synthetic_labels(data:pd.DataFrame(), min_num_labels:int=100):\n",
    "    '''\n",
    "    Returns a new DataFrame with synthetic data. Parameters are input dataframe, minimum number of labels, \n",
    "    and list of levels that sythetic data should be applied to.\n",
    "\n",
    "            Parameters:\n",
    "                    data (pd.DataFrame()): Pandas DataFrame\n",
    "                    min_num_labels (int): Decimal Integer, with default value of 100\n",
    "\n",
    "            Returns:\n",
    "                    result_df (pd.DataFrame): Resulting dataframe with synthetic labels\n",
    "    '''\n",
    "    issue_cat_list = [item for item in data['labels'].value_counts().items()]\n",
    "    for issue_cat, issue_num in issue_cat_list:\n",
    "        if issue_num < min_num_labels:\n",
    "            synth_add_num = min_num_labels-issue_num\n",
    "            real_cat_list = data[data['labels']==issue_cat].index.tolist()\n",
    "            real_cat_num = len(real_cat_list)\n",
    "            quotient = int(synth_add_num/real_cat_num)\n",
    "            remainder = synth_add_num%real_cat_num\n",
    "            remainder_count = 1\n",
    "            for real_cat_item in real_cat_list:\n",
    "                if quotient > 0:\n",
    "                    for count in range(quotient):\n",
    "                        temp_row = data.iloc[real_cat_item]\n",
    "                        temp_row['data_type'] = 'synthetic'\n",
    "                        temp_row['comments'] = aug1.augment(temp_row['comments'])\n",
    "                        data = data.append(temp_row,ignore_index=True)\n",
    "                if (remainder > 0) & (remainder_count<=remainder):\n",
    "                        temp_row = data.iloc[real_cat_item]\n",
    "                        temp_row['data_type'] = 'synthetic'\n",
    "                        temp_row['comments'] = aug1.augment(temp_row['comments'])\n",
    "                        data = data.append(temp_row,ignore_index=True)\n",
    "                        remainder_count=remainder_count+1\n",
    "  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "rawFolder = 'Smoketests/'\n",
    "preprocesedFolder = 'Preprocessed/'\n",
    "for entry in os.listdir(rawFolder):\n",
    "    if 'csv' in entry or 'parquet' in entry:\n",
    "        files.append(entry)\n",
    "dataset = files[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "Language.factory(\"language_detector\", func = get_lang_detector)\n",
    "nlp.add_pipe('language_detector', last = True)\n",
    "# pd.set_option('display.max_columns', None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90724955bdf944a28d5baf8274427036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/9375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 1s, sys: 2.36 s, total: 5min 3s\n",
      "Wall time: 5min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9345, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df = (pd.read_parquet(dataset)\n",
    "        .query(\"CallCentreRep == 'Web Comment'\")\n",
    "        .reset_index(drop=True)\n",
    "        .filter(regex='[Mm]sg[DL]|\\w+\\W', axis = 1)\n",
    "        .assign(cats = lambda x: get_cats(x.iloc[:,4:]))\n",
    "        .filter(items = ['MsgDate', 'MsgLevelDescript', 'MsgLevel', 'MsgDetails', 'cats'])\n",
    "        .rename(columns = {'MsgDetails': 'comments', 'MsgLevelDescript': 'labels'})\n",
    "        .fillna({\"comments\": 'No comment found'})# imputing missing values\n",
    "      # .query(\"description == 'Web Comment' & issue != 'Lv 4 - Hang Up/Wrong Number/Disconnect/Prank'\")# removing phone comments\n",
    "      # .drop_duplicates(subset='case_id')# removing duplicate cases\n",
    "        .reset_index(drop = True)\n",
    "        .pipe(fix_ShortComments)# imputing comments with <=3 characters\n",
    "        .assign(comments = lambda x: text_cleaning(x, 'comments')# cleaning text by removing: html, unicode, punctuation, etc\n",
    "                , labels = lambda x: x['labels'].replace('Level\\s\\d\\s\\W', '', regex = True)# Removes 'Level - D' from each row\n",
    "                , Doc = lambda x: generate_Doc(x, 'comments')# generating nlp objects\n",
    "                , NumSents = lambda x: count_Sents(x, 'Doc')# counting sentences per comment (spaCy)\n",
    "                , NumWords = lambda x: x['Doc'].apply(len)# counting the words per comment\n",
    "                , Lang = lambda x: get_Lang(x, 'Doc', info = 'language')# extracting language from nlp object\n",
    "                , Score = lambda x: get_Lang(x, 'Doc', info = 'score')# extracting language score from nlp object\n",
    "                )\n",
    "        .pipe(remove_ShortComments)# Removes comments whose length is < 2 (mostly empty/one-character comments\n",
    "        .query(\"Lang != 'es' & Score > 0.70\")\n",
    "        .pipe(delete_AgentComments)\n",
    "      )\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MODEL_DIR\"] = 'model'\n",
    "model_dir = 'model/'\n",
    "aug1 = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"insert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['issues'] = le.fit_transform(df['MsgLevelDescript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = df[['comments', 'labels']].copy()\n",
    "output_df['data_type'] = 'real'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4d 26min, sys: 2h 10min 40s, total: 4d 2h 36min 41s\n",
      "Wall time: 12h 44min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_ready_df = create_synthetic_labels(output_df, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ready_df.comments = output_ready_df.comments.apply(''.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>labels</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[sent sunday, july 29 to 31, 2022 731 pm to fa...</td>\n",
       "      <td>DQ Cake</td>\n",
       "      <td>synthetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[general corporate request yes i know that i s...</td>\n",
       "      <td>Other Request/Suggestion</td>\n",
       "      <td>synthetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[general item of sales promotional notice corp...</td>\n",
       "      <td>Dilly Bar</td>\n",
       "      <td>synthetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sent sunday, july 31, 2022 741 pm to fan relat...</td>\n",
       "      <td>DQ Mobile App</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>points and rewards please add my purchase to m...</td>\n",
       "      <td>DQ Mobile App Points</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  \\\n",
       "0  [sent sunday, july 29 to 31, 2022 731 pm to fa...   \n",
       "1  [general corporate request yes i know that i s...   \n",
       "2  [general item of sales promotional notice corp...   \n",
       "3  sent sunday, july 31, 2022 741 pm to fan relat...   \n",
       "4  points and rewards please add my purchase to m...   \n",
       "\n",
       "                      labels  data_type  \n",
       "0                    DQ Cake  synthetic  \n",
       "1   Other Request/Suggestion  synthetic  \n",
       "2                  Dilly Bar  synthetic  \n",
       "3              DQ Mobile App       real  \n",
       "4       DQ Mobile App Points       real  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ready_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ready_df.to_parquet(preprocesedFolder+dataset,engine = 'pyarrow',compression = 'gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
